import cv2
import torch
import time
import sys
import os
from threading import Thread
from queue import Queue, Empty
from ultralytics import YOLO
from datetime import datetime

# --- 1. é…ç½®å‚æ•° ---
# å»ºè®®ç¡®ä¿è·¯å¾„æŒ‡å‘ä½ è¡¨ç°æœ€å¥½çš„ 9604 æ–‡ä»¶å¤¹æƒé‡
VIDEO_PATH = "3.mp4"
MODEL_PATH = "DJI_VisDrone/yolo12n_3070Ti_1280/weights/1280best.pt"
OUTPUT_PATH = "Deep_Optimized_DJI_1280test01.mp4"


class DJIProcessor:
    def __init__(self, video_path, model_path):
        # ç¡¬ä»¶åŠ é€Ÿé…ç½®
        self.device = torch.device("cuda:0")
        # åŠ è½½æ¨¡å‹å¹¶åˆå§‹åŒ–
        self.model = YOLO(model_path).to(self.device)

        # è§†é¢‘å…ƒæ•°æ®æå–
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

        self.width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.fps = cap.get(cv2.CAP_PROP_FPS)
        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        # é˜Ÿåˆ—å®šä¹‰ï¼šé’ˆå¯¹ 16G å†…å­˜é™åˆ¶ï¼Œè®¾ä¸º 30 å¸§å®‰å…¨é˜ˆå€¼ï¼Œé˜²æ­¢ OOM
        self.result_queue = Queue(maxsize=30)
        self.stopped = False

    def inference_engine(self):
        """æ ¸å¿ƒæ¨ç†å¼•æ“ï¼šè´Ÿè´£ GPU é«˜é€Ÿè¿ç®—"""
        # half=True å¼€å¯ FP16ï¼Œåœ¨ 3070Ti ä¸Šå¯æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶ç¿»å€é€Ÿåº¦
        results_gen = self.model.predict(
            source=VIDEO_PATH,
            imgsz=1280,
            device=self.device,
            stream=True,
            half=True,  # å¼ºçƒˆå»ºè®®å¼€å¯ï¼š3070Ti ä¸‹ä¸æŸå¤±ç²¾åº¦ä¸”æ˜¾è‘—é™æ¸©ã€æé€Ÿ
            conf=0.15,  # æƒè¡¡å€¼ï¼š0.15 å¯èƒ½ä¼šå¯¼è‡´ç”»é¢èƒŒæ™¯â€œé—ªçƒâ€è™šè­¦ï¼Œ0.2 æ›´ç¨³
            iou=0.9,  # ä¿æŒ 0.7ï¼šå¯†é›†åœºæ™¯å¿…é¡»æ”¾å®½ IOUï¼Œé˜²æ­¢å¹¶æ’çš„äººè¢«å‰”é™¤
            agnostic_nms=False,  # å…³é”®ï¼šè®¾ä¸º Falseã€‚å¦‚æœè¡Œäººå’Œè‡ªè¡Œè½¦é‡å ï¼Œä¸¤è€…éƒ½ä¼šä¿ç•™
            max_det=2000,  # å¿…é¡»è°ƒå¤§ï¼šVisDrone 4K åœºæ™¯ä¸€å¸§å¯èƒ½æœ‰å‡ ç™¾ä¸ªç›®æ ‡ï¼Œé»˜è®¤ 300 å¯èƒ½ä¸å¤Ÿ
            augment=False,  # å®æ—¶æ¨ç†å»ºè®® Falseï¼Œå¦‚æœè¿½æ±‚æè‡´ç²¾åº¦ä¸”ä¸è®¡æˆæœ¬å¯è®¾ä¸º True
            classes=None,  # å¦‚æœä½ åªå…³å¿ƒè½¦å’Œäººï¼Œå¯ä»¥æŒ‡å®šç±»åˆ«ç´¢å¼•ï¼Œå¦‚ [0, 1, 2]
            verbose=False
        )

        for result in results_gen:
            if self.stopped:
                break
            # é˜»å¡å¼å…¥é˜Ÿï¼Œå¦‚æœç»˜å›¾å¤ªæ…¢ï¼ŒGPU ä¼šç­‰å¾… CPU
            self.result_queue.put(result)

        self.stopped = True

    def video_writer_engine(self):
        """å†™å…¥å¼•æ“ï¼šè´Ÿè´£ CPU ç»˜å›¾ä¸è§†é¢‘ç¼–ç ä¿å­˜"""
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(OUTPUT_PATH, fourcc, self.fps, (self.width, self.height))

        processed_count = 0
        start_time = time.time()

        while not (self.stopped and self.result_queue.empty()):
            try:
                # è®¾ç½®è¶…æ—¶é˜²æ­¢åœ¨é˜Ÿåˆ—æœ«å°¾æ­»é”
                result = self.result_queue.get(timeout=2)

                # ç»˜åˆ¶ç›®æ ‡æ¡†ï¼š1280px ä¸‹çº¿å®½è®¾ä¸º 1ï¼Œé¿å…é®æŒ¡å¾®å°ç›®æ ‡
                annotated_frame = result.plot(
                    line_width=1,
                    labels=True,
                    conf=True
                )

                # å åŠ å®æ—¶æ£€æµ‹ç»Ÿè®¡
                total_objects = len(result.boxes)
                cv2.putText(annotated_frame, f"Detections: {total_objects}", (40, 70),
                            cv2.FONT_HERSHEY_DUPLEX, 1.5, (0, 255, 0), 2)

                out.write(annotated_frame)
                processed_count += 1

                # æ€§èƒ½å®æ—¶é¢æ¿
                if processed_count % 15 == 0:
                    elapsed = time.time() - start_time
                    current_fps = processed_count / elapsed
                    vram_used = torch.cuda.memory_reserved() / 1e9
                    sys.stdout.write(f"\rğŸš€ 1280px æ¨ç†ä¸­: {processed_count}/{self.total_frames} | "
                                     f"é€Ÿåº¦: {current_fps:.1f} FPS | æ˜¾å­˜: {vram_used:.2f}GB")
                    sys.stdout.flush()

            except Empty:
                continue

        out.release()
        print(f"\n\nâœ¨ å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š å¹³å‡å¤„ç†é€Ÿåº¦: {processed_count / (time.time() - start_time):.2f} FPS")
        print(f"ğŸ“ ç»“æœè·¯å¾„: {os.path.abspath(OUTPUT_PATH)}")

    def run(self):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¯åŠ¨ 1280px é«˜æ€§èƒ½æ¨ç†æ¨¡å¼...")

        # ä»…å¼€å¯åŒçº¿ç¨‹ï¼šGPU æ¨ç†çº¿ç¨‹ + CPU å†™å…¥çº¿ç¨‹
        # predict(stream=True) å·²åŒ…å«é«˜æ•ˆå–å¸§é€»è¾‘ï¼Œä¸å†éœ€è¦å•ç‹¬çš„ reader çº¿ç¨‹
        t_infer = Thread(target=self.inference_engine)
        t_write = Thread(target=self.video_writer_engine)

        t_infer.start()
        t_write.start()

        t_infer.join()
        t_write.join()


if __name__ == "__main__":
    # å¼ºåˆ¶æ¸…ç†ä¸€æ¬¡æ˜¾å­˜ç¢ç‰‡
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    processor = DJIProcessor(VIDEO_PATH, MODEL_PATH)
    processor.run()